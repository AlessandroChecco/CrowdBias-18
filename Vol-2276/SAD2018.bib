@comment{It is strongly recommended to use BibLaTeX for these entries}

@inproceedings{VincentMestre:SAD_CROWDBIAS_HCOMP2018,
title = {Crowdsourced Measure of News Articles Bias: Assessing Contributors' Reliability},
author = {Emmanuel Vincent and Maria Mestre},
pages = {1--10},
url = {http://ceur-ws.org/Vol-2276/#paper-1},
crossref = {SAD_CROWDBIAS_HCOMP2018},
}

@inproceedings{DumitracheEtAl:SAD_CROWDBIAS_HCOMP2018,
title = {CrowdTruth 2.0: Quality Metrics for Crowdsourcing with Disagreement},
author = {Anca Dumitrache and Oana Inel and Lora Aroyo and Benjamin Timmermans and Chris Welty},
pages = {11--18},
url = {http://ceur-ws.org/Vol-2276/#paper-2},
crossref = {SAD_CROWDBIAS_HCOMP2018},
}

@inproceedings{PalomakiRhinehartTseng:SAD_CROWDBIAS_HCOMP2018,
title = {A Case for a Range of Acceptable Annotations},
author = {Jennimaria Palomaki and Olivia Rhinehart and Michael Tseng},
pages = {19--31},
url = {http://ceur-ws.org/Vol-2276/#paper-3},
crossref = {SAD_CROWDBIAS_HCOMP2018},
}

@inproceedings{JongEtAl:SAD_CROWDBIAS_HCOMP2018,
title = {CaptureBias: Using Ambiguity to Support Media Scientists in News Videos Bias Detection},
author = {Markus de Jong and Panagiotis Mavridis and Lora Aroyo and Alessandro Bozzon and Jesse de Vos and Johan Oomen and Antoaneta Dimitrova and Alec Badenoch},
pages = {32--40},
url = {http://ceur-ws.org/Vol-2276/#paper-4},
crossref = {SAD_CROWDBIAS_HCOMP2018},
}

@inproceedings{WarrenHayes:SAD_CROWDBIAS_HCOMP2018,
title = {Bounding Ambiguity: Experiences with an Image Annotation System},
author = {Margaret Warren and Pat Hayes},
pages = {41--54},
url = {http://ceur-ws.org/Vol-2276/#paper-5},
crossref = {SAD_CROWDBIAS_HCOMP2018},
}

@inproceedings{SchaekermannEtAl:SAD_CROWDBIAS_HCOMP2018,
title = {Expert Disagreement in Sequential Labeling: A Case Study on Adjudication in Medical Time Series Analysis},
author = {Mike Schaekermann and Edith Law and Kate Larson and Andrew Lim},
pages = {55--66},
url = {http://ceur-ws.org/Vol-2276/#paper-6},
crossref = {SAD_CROWDBIAS_HCOMP2018},
}

@inproceedings{BalaynEtAl:SAD_CROWDBIAS_HCOMP2018,
title = {Characterising and Mitigating Aggregation-Bias in Crowdsourced Toxicity Annotations},
author = {Agathe Balayn and Panagiotis Mavridis and Alessandro Bozzon and Benjamin Timmermans and Zoltán Szlávik},
pages = {67--71},
url = {http://ceur-ws.org/Vol-2276/#paper-7},
crossref = {SAD_CROWDBIAS_HCOMP2018},
}

@inproceedings{VougiouklisEtAl:SAD_CROWDBIAS_HCOMP2018,
title = {How Biased Is Your NLG Evaluation?},
author = {Pavlos Vougiouklis and Eddy Maddalena and Jonathon Hare and Elena Simperl},
pages = {72--77},
url = {http://ceur-ws.org/Vol-2276/#paper-8},
crossref = {SAD_CROWDBIAS_HCOMP2018},
}

@inproceedings{HubeEtAl:SAD_CROWDBIAS_HCOMP2018,
title = {LimitBias! Measuring Worker Biases in the Crowdsourced Collection of Subjective Judgments},
author = {Christoph Hube and Besnik Fetahu and Ujwal Gadiraju},
pages = {78--82},
url = {http://ceur-ws.org/Vol-2276/#paper-9},
crossref = {SAD_CROWDBIAS_HCOMP2018},
}


@inproceedings{QaroutEtAl:SAD_CROWDBIAS_HCOMP2018,
title = {Investigating Stability and Reliability of Crowdsourcing Output},
author = {Rehab Kamal Qarout and Alessandro Checco and Kalina Bontcheva},
pages = {83--87},
url = {http://ceur-ws.org/Vol-2276/#paper-10},
crossref = {SAD_CROWDBIAS_HCOMP2018},
}

@inproceedings{MavridisEtAl:SAD_CROWDBIAS_HCOMP2018,
title = {A Human in the Loop Approach to Capture Bias and Support Media Scientists in News Video Analysis},
author = {Panagiotis Mavridis and Markus de Jong and Lora Aroyo and Alessandro Bozzon and Jesse de Vos and Johan Oomen and Antoaneta Dimitrova and Alec Badenoch},
pages = {88--92},
url = {http://ceur-ws.org/Vol-2276/#paper-11},
crossref = {SAD_CROWDBIAS_HCOMP2018},
}

@inproceedings{BarzEtAl:SAD_CROWDBIAS_HCOMP2018,
title = {Device-Type Influence in Crowd-based Natural Language Translation Tasks},
author = {Michael Barz and Neslihan Büyükdemircioglu and Rikhu Prasad Surya and Tim Polzehl and Daniel Sonntag},
pages = {93--97},
url = {http://ceur-ws.org/Vol-2276/#paper-12},
crossref = {SAD_CROWDBIAS_HCOMP2018},
}

@proceedings{SAD_CROWDBIAS_HCOMP2018,
booktitle = {1st Workshop on Subjectivity, Ambiguity and Disagreement (SAD) in Crowdsourcing 2018, and CrowdBias'18: Disentangling the Relation Between Crowdsourcing and Bias Management},
title = {Proceedings of the 1st Workshop on Subjectivity, Ambiguity and Disagreement (SAD) in Crowdsourcing 2018, and the 1st Workshop CrowdBias'18: Disentangling the Relation Between Crowdsourcing and Bias Management},
year = 2018,
editor = {Alessandro Checco and Gianluca Demartini and Ujwal Gadiraju and Cristina Sarasua and Lora Aroyo and Anca Dumitrache and Praveen Paritosh and Alex Quinn and Chris Welty}, 
number = 2276,
series = {CEUR Workshop Proceedings},
address = {Aachen},
issn = {1613-0073},
url = {http://ceur-ws.org/Vol-2276/},
venue = {Zürich, Switzerland},
eventdate = {2018-07-05},
}


